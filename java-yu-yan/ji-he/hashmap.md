# HaspMap扩容是怎样扩容的，为什么都是2的N次幂的大小。

### 为什么都是2的N次幂的大小

HashMap有两个参数影响其性能：初始容量和加载因子。默认初始容量是16，加载因子是0.75。

HashMap中计算key的哈希值时，计算方法：

```text
(h = key.hashCode()) ^ (h >>> 16)
```

确定键值对应该存放在哪个桶中时，计算方法为：

```text
(n - 1) & hash
```

当数组长度为2的n次幂的时候，不同的key算得得index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。

TODO：为什么？这种位运算的神奇之处在哪？

### 怎样扩容

扩容时机：

当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。

大体过程：

先将table数组长度变为原来的2倍，然后将table中的键值对重新分配到各个桶中：循环原table，注意的是，循环次数为数组table的长度，而不是HashMap的size（键值对的个数）；在循环体中，对原table中任意一个位置，会判断该位置的结点类型：只有一个、链表、红黑树，然后分类进行重哈希。

如果原table的该位置处的桶结构为链表，则该链表会保留元素顺序：它指的是，假如链表中两个结点a和b，有a.next=b或者a.next.next...=b，那么在重哈希以后，如果a和b仍然在同一链表中，依然会保持a.next=b或者a.next.next...=b的关系；当然只是顺序，next的次数与之前是不同的。

## HashMap在高并发下如果没有处理线程安全会有怎样的安全隐患，具体表现是什么。

①put后可能导致get死循环，具体表现为CPU使用率100%：

②put的时候可能导致元素丢失：执行addEntry\(hash, key, value, i\)，如果有产生哈希碰撞，导致两个线程得到同样的bucketIndex去存储，就可能会出现覆盖丢失的情况：

TODO：具体的代码解释

内容参考：

[HashMap在并发下可能出现的问题分析](http://www.cnblogs.com/binyue/p/3726403.html)

